% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={A note on combining evidence from different GWAS studies for binary traits},
  pdfauthor={Ziang Zhang, Lei Sun},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{setspace}\doublespacing
\usepackage{amsmath,amsthm, amssymb}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\title{\textbf{A note on combining evidence from different GWAS studies for
binary traits}}
\author{Ziang Zhang, Lei Sun}
\date{}

\begin{document}
\maketitle

\newcommand{\p}{\text{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}

\hypertarget{introduction}{%
\section{Introduction:}\label{introduction}}

For most human traits, genetic effects from specific SNP only have small
effect sizes (Evangelou and Ioannidis 2013). Therefore, practitioners
often aggregate the GWAS results from different studies through
meta-analysis or mega-analysis in order to achieve higher power.
Aggregation through p-values is a commonly used example of meta-analysis
method. For example when there are several different datasets,
practitioners sometimes will first carry out a comprehensive study using
one dataset, and only follow up with the SNPs that have the highest
significance levels (Begum et al. 2012). Another example would be
multi-trait analysis which contains a joint analysis of multiple related
traits, in order to boost the statistical power (Turley et al. 2018). In
this note, we would like to consider some problems that practitioners
may encounter when applying such procedures on the analysis of binary
traits.

With examples of Wald test, we presented the phenomenon that the same
hypothesis test on different binary traits can have very different
distributions of p-value under the alternative hypothesis, hence very
different powers, even if the two traits have the same true SNP effect
and are analyzed using the same dataset. The same conclusion will still
hold if the practitioner uses likelihood ratio test or score test, and
we choose to focus on Wald test in this note because of its
computational convenience.

In the next section, we will give a brief overview of Wald test for
Generalized Linear Models, with a short explanation of the rationale
behind the phenomenon we mentioned above. Then, we will follow with two
simulation studies to illustrate how this phenomenon is affecting the
analysis of binary trait, but not the analysis of continuous trait.

\hypertarget{wald-test-for-generalized-linear-models}{%
\section{Wald test for Generalized Linear
Models:}\label{wald-test-for-generalized-linear-models}}

Assume the generalized linear model (glm) has the following form:
\[\mathbb{E}(Y|X) = \mu = g^{-1}(\beta_0 + \beta_G G + \beta_E E) = g^{-1}(\eta),\]
where \(g(.)\) is a specific link function connecting the linear
predictor \(\eta\) with the mean function of \(Y\). The design matrix
\(X\) has rows \(\{(1,G_i,E_i)_{i=1}^n\}\), where \(n\) is the sample
size and the first column of 1 is for the intercept \(\beta_0\). For the
simplicity of notations, we will use
\(\beta=(\beta_0,\beta_G,\beta_E)^T\) to denote both the vector of all
regression parameters and the vector of their true values.

In this case, the fisher information matrix at \(\beta\) can be written
as \[I_n(\beta) = X^TW(\beta) X,\] where \(W(\beta)\) is a diagonal
matrix with each diagonal term depending on the value of \(\beta\)
unless \(g\) is identity function. Specifically, the \(i^{th}\) diagonal
term of W can be computed as
\[w_i=(\frac{\partial u_i}{\partial\eta_i})^2/\text{Var}(Y_i|X).\]

If the question of interest is to test the hypothesis \(H_0: \beta_G=0\)
using Wald test, the test statistic can be written as
\[Z = \frac{\hat{\beta}_G}{\sqrt{I_n^{-1}(\hat{\beta})_{[3,3]}}},\]
where \(I_n^{-1}(\hat{\beta})_{[3,3]}\) denotes the third diagonal term
of the matrix \(I_n^{-1}(\hat{\beta})\) and \(\hat{\beta}\) is the MLE
estimate. Under the null hypothesis, \(Z^2\) asymptotically follows a
Chi-Square distribution with 1 degree of freedom.

Under the alternative hypothesis that \(\beta_G \neq 0\), the
non-centrality parameter of the squared Wald test statistics above can
be computed as \[\frac{\beta_G^2}{I_n^{-1}(\beta)_{[3,3]}}.\] Since
\(I_n^{-1}(\beta)_{[3,3]}\) will not solely depend on \(\beta_G\) unless
\(g\) is identity. Therefore the power function of this Wald test will
not only depend on \(\beta_G\), but the whole vector \(\beta\).

More specifically, let
\(d=-\frac{\beta_G}{\sqrt{I_n^{-1}(\beta)_{[3,3]}}}\), then the
theoretical power of this Wald test at \(\beta\) can be computed as
\[1-\Phi(d+z_{a/2})+\Phi(d-z_{a/2}),\] where \(\Phi\) is the CDF of
standard normal and \(z_{a/2}\) is the \(a/2\) quantile of standard
normal.

In summary, this means that if we utilize Poisson regression to analyze
count traits (e.g.~number of cancers) or Logistic/Probit regression to
analyze binary traits (e.g.~disease status), powers of Wald test from
different studies can be dramatically different, even if the two studies
have the same effect size (i.e.~\(|\beta_G|\)) and the same set of
covariates \(\{G_{i},E_{i}\}_{i=1}^n\). The rationale behind this is
actually the classical contrast between
\textbf{statistical significance} measured by p-values and
\textbf{practical significance} measured by the size of the effect
\(|\beta_G|\). In Wald test, p-values are determined by both the
practical significance \(|\beta_G|\) and the standard error
\(\sqrt{I_n^{-1}(\beta)_{[3,3]}}\). Since the standard errors of the MLE
estimator will be different on the two studies, the conclusion drawn
from statistical significance may be inconsistent with the practical
significance of effects in the two studies.

\hypertarget{difference-between-continuous-trait-and-binary-trait}{%
\section{Difference between continuous trait and binary
trait:}\label{difference-between-continuous-trait-and-binary-trait}}

For the analysis of continuous trait, a natural option would be using
ordinary Gaussian linear regression model:
\[Y = \beta_0 + \beta_G G + \beta_E E+\epsilon,\] where
\(\epsilon \sim N(0,\sigma)\).

In this case, suppose the question of interest is testing \(\beta_G=0\),
the Wald test will have non-centrality parameter being
\[\frac{\beta_G^2}{\sigma^2[X^TX]^{-1}_{[33]}}.\]

Because the (inverse) information matrix \(\sigma^2[X^TX]^{-1}\) only
depends on the nuisance parameter \(\sigma\), power of this test will
not change as \(\beta_0\) or \(\beta_E\) change, as long as the nuisance
parameter is the same. This implies, if a SNP \(G\) has true effect
\(\beta_G\) being constant across two traits and assume \(\sigma\) being
the same, then the power of testing \(\beta_G = 0\) will be constant
across two traits as well. Therefore, the aggregation of evidences
across traits will be straightforward and smaller p value can be
associated with larger SNP effect.

On the other hand, if the target is to combine evidence across several
binary traits, the result will no longer be as straightforward. If
traits are generated from the following logistic regression model:
\[\text{logit}(\text{P}(Y=1|X)) = \beta_0 + \beta_G G + \beta_E E,\] In
this case, the corresponding non-centrality parameter of testing
\(\beta_G=0\) becomes \[\frac{\beta_G}{[X^TW_\beta X]^{-1}_{[33]}}.\]

Since \(g^{-1}(\eta)= \frac{\exp(\eta)}{1+\exp(\eta)}\) for logistic
regression, we can compute the \(i^{th}\) term of the diagonal matrix
\(W_\beta\) as
\[[W_\beta]_{ii} = f(\eta_i)=\frac{\exp(-\eta_i)}{(1+\exp(-\eta_i))^2}.\]
The function \(f(.)\) represents the density function of standard
logistic distribution, and the term \(\eta_i\) is the i-th linear
predictor. This implies the non-centrality parameter, hence the power
will depend on every parameter in the model, not just on the parameter
of interest \(\beta_G\).

To better understand the influence of \(\beta_E\) on power of the Wald
test for \(\beta_G\). Note that
\[\sigma_\eta^2:=\text{Var}(\eta_i) = \beta_E^2\sigma_1^2+\beta_G^2\sigma_2^2 = c_1 +c_2\beta_E^2,\]
where we use \(c_1\) and \(c_2\) to denote constant terms that do not
depend on \(\beta_E\). Assume that the matrix \(X^TX \approx \Sigma_X\),
and \(\eta_i\sim N(0,\sigma_\eta^2)\) (with density \(\phi_\eta\)),
where \(\Sigma_X\) is a matrix that does not depend on \(\beta_E\). We
will use the following approximation to the fisher information
matrix:\[I = X^TW_\beta X \approx \mu_w \Sigma_X,\] where
\(\mu_w = \mathbb{E}(w_i)\). Hence, we know that
\[I^{-1}_{[2,2]} \propto \frac{1}{\mu_w}.\]

To compute \(\mu_w\), we can approximate the standard logistic density
\(f(.)\) with density of \(N(0,\frac{\pi}{\sqrt{3}})\) (denoted by
\(\phi_1\)). Therefore, we have
\[\mu_w = E(f(\eta_i)) \approx \int\phi_1(x)\phi_{\eta}(x)dx = \frac{1}{\sqrt{2\pi(\sigma_\eta^2+\frac{\pi^2}{3})}} \overset{approxi}{\propto} \frac{1}{|\beta_E|}.\]
Hence \(I^{-1}_{[2,2]} \overset{approxi}{\propto} |\beta_E|\), which
means that we expect the power of testing \(\beta_G\) decreases as
\(|\beta_E|\) grows.

An important consequence of this phenomenon is that, the magnitude of p
values (statistical significance) will not reflect the magnitude of the
SNP effects (practical significance), even if the two studies are
carried out on the same set of individuals. For example, if a SNP has
effect \(\beta_G\) on both traits, it may show significance on only one
trait because of the difference in the covariate effect such as age or
sex.

\hypertarget{simulation-with-gaussian-linear-regression-model}{%
\section{Simulation with Gaussian linear regression
model:}\label{simulation-with-gaussian-linear-regression-model}}

Assume a dataset of size \(n=1000\) has been collected. The dataset
contains the information of a continuous covariate E, a SNP G, and two
continuous traits (A and B). Without loss of generality, assume this SNP
follows hardy weinberg equilibrium (HWE) with minor allele frequency
(MAF) \(0.3\), and the covariate \(E\) has been centered such that
\(E\sim N(0,3)\).

Furthermore, we assume that the generating models for each trait are the
followings: \[\textbf{A}:Y = -0.5 + 0.3G + 0.8E + \epsilon,\]
\[\textbf{B}: Y = -0.5 + 0.3G + 0.1E + \epsilon,\] where the noise term
\(\epsilon\) follows \(N(0,3)\) in both models.

To test the null hypothesis \(H_0: \beta_G = 0\), a Wald test can be
carried out for each trait (with \(\alpha=0.05\) for simplicity). Using
the formula from above, we can compute the theoretical power of each
trait:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### Simulated the common E and G}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DataTypeTok{sample.kind =} \StringTok{"Rounding"}\NormalTok{)}
\NormalTok{N <-}\StringTok{ }\DecValTok{1000}
\NormalTok{G <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DataTypeTok{size =}\NormalTok{ N, }\DataTypeTok{replace =}\NormalTok{ T, }\DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.49}\NormalTok{,}\FloatTok{0.42}\NormalTok{,}\FloatTok{0.09}\NormalTok{))}
\NormalTok{E <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(N,}\DataTypeTok{sd =} \DecValTok{3}\NormalTok{)}

\CommentTok{### Simulate each trait's disease status based on E and G}
\CommentTok{## A:}
\NormalTok{beta0 <-}\StringTok{ }\FloatTok{-0.5}
\NormalTok{betaE <-}\StringTok{ }\FloatTok{0.8}
\NormalTok{betaG <-}\StringTok{ }\FloatTok{0.3}
\NormalTok{yA <-}\StringTok{ }\NormalTok{beta0 }\OperatorTok{+}\StringTok{ }\NormalTok{betaG}\OperatorTok{*}\NormalTok{G }\OperatorTok{+}\StringTok{ }\NormalTok{betaE}\OperatorTok{*}\NormalTok{E }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(N,}\DataTypeTok{sd =} \DecValTok{3}\NormalTok{)}

\CommentTok{## B:}
\NormalTok{beta0 <-}\StringTok{ }\FloatTok{-0.5}
\NormalTok{betaE <-}\StringTok{ }\FloatTok{0.1}
\NormalTok{betaG <-}\StringTok{ }\FloatTok{0.3}
\NormalTok{yB <-}\StringTok{ }\NormalTok{beta0 }\OperatorTok{+}\StringTok{ }\NormalTok{betaG}\OperatorTok{*}\NormalTok{G }\OperatorTok{+}\StringTok{ }\NormalTok{betaE}\OperatorTok{*}\NormalTok{E }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(N, }\DataTypeTok{sd =} \DecValTok{3}\NormalTok{)}

\CommentTok{## A:}
\NormalTok{beta0 <-}\StringTok{ }\FloatTok{-0.5}
\NormalTok{betaE <-}\StringTok{ }\FloatTok{0.8}
\NormalTok{betaG <-}\StringTok{ }\FloatTok{0.3}
\CommentTok{### Theoretical Power}
\NormalTok{mod_A <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(yA}\OperatorTok{~}\NormalTok{E }\OperatorTok{+}\StringTok{ }\NormalTok{G)}
\CommentTok{#### Get the design matrix:}
\NormalTok{X <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,N),mod_A}\OperatorTok{$}\NormalTok{model[,}\OperatorTok{-}\DecValTok{1}\NormalTok{])}
\CommentTok{### Compute the weight matrix W:}
\NormalTok{beta <-}\StringTok{ }\KeywordTok{c}\NormalTok{(beta0,betaE,betaG)}
\NormalTok{I <-}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{9}\NormalTok{)}\OperatorTok{*}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{t}\NormalTok{(X)) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(X)}
\CommentTok{#### Invert to get the true covariance matrix }
\NormalTok{V <-}\StringTok{ }\KeywordTok{solve}\NormalTok{(I)}
\CommentTok{### Compute the power function:}
\NormalTok{delta <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{V[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{])}\OperatorTok{*}\NormalTok{(}\DecValTok{0}\OperatorTok{-}\NormalTok{beta[}\DecValTok{3}\NormalTok{])}
\NormalTok{alpha <-}\StringTok{ }\FloatTok{0.05}
\NormalTok{Power_A <-}\StringTok{ }\DecValTok{1}\OperatorTok{-}\StringTok{ }\KeywordTok{pnorm}\NormalTok{(delta }\OperatorTok{-}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{)) }\OperatorTok{+}\StringTok{ }\KeywordTok{pnorm}\NormalTok{(delta }\OperatorTok{+}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{))}
\NormalTok{Power_A}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.548751
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## B:}
\NormalTok{beta0 <-}\StringTok{ }\FloatTok{-0.5}
\NormalTok{betaE <-}\StringTok{ }\FloatTok{0.8}
\NormalTok{betaG <-}\StringTok{ }\FloatTok{0.3}
\CommentTok{### Theoretical Power}
\NormalTok{mod_B <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(yB}\OperatorTok{~}\NormalTok{E }\OperatorTok{+}\StringTok{ }\NormalTok{G)}
\CommentTok{#### Get the design matrix:}
\NormalTok{X <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,N),mod_B}\OperatorTok{$}\NormalTok{model[,}\OperatorTok{-}\DecValTok{1}\NormalTok{])}
\CommentTok{### Compute the weight matrix W:}
\NormalTok{beta <-}\StringTok{ }\KeywordTok{c}\NormalTok{(beta0,betaE,betaG)}
\NormalTok{I <-}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{9}\NormalTok{)}\OperatorTok{*}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{t}\NormalTok{(X)) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(X)}
\CommentTok{#### Invert to get the true covariance matrix }
\NormalTok{V <-}\StringTok{ }\KeywordTok{solve}\NormalTok{(I)}
\CommentTok{### Compute the power function:}
\NormalTok{delta <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{V[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{])}\OperatorTok{*}\NormalTok{(}\DecValTok{0}\OperatorTok{-}\NormalTok{beta[}\DecValTok{3}\NormalTok{])}
\NormalTok{alpha <-}\StringTok{ }\FloatTok{0.05}
\NormalTok{Power_B <-}\StringTok{ }\DecValTok{1}\OperatorTok{-}\StringTok{ }\KeywordTok{pnorm}\NormalTok{(delta }\OperatorTok{-}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{)) }\OperatorTok{+}\StringTok{ }\KeywordTok{pnorm}\NormalTok{(delta }\OperatorTok{+}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{))}
\NormalTok{Power_B}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.548751
\end{verbatim}

Based on the formula from previous section, we can compute the power to
be 0.549 in both studies. As we expected, since in both traits the SNP
effect is the same, powers should be the same for the two traits as
well. To make sure the computed theoretical powers are indeed correct,
we can compare them with empirical powers obtained from repeated
simulations (K = 2000):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12345}\NormalTok{,}\DataTypeTok{sample.kind =} \StringTok{"Rounding"}\NormalTok{)}
\CommentTok{## A:}
\NormalTok{beta0 <-}\StringTok{ }\FloatTok{-0.5}
\NormalTok{betaE <-}\StringTok{ }\FloatTok{0.8}
\NormalTok{betaG <-}\StringTok{ }\FloatTok{0.3}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{2000}\NormalTok{) \{}
\NormalTok{  yA <-}\StringTok{ }\NormalTok{beta0 }\OperatorTok{+}\StringTok{ }\NormalTok{betaG}\OperatorTok{*}\NormalTok{G }\OperatorTok{+}\StringTok{ }\NormalTok{betaE}\OperatorTok{*}\NormalTok{E }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(N, }\DataTypeTok{sd =} \DecValTok{3}\NormalTok{)}
\NormalTok{  mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(yA}\OperatorTok{~}\NormalTok{E}\OperatorTok{+}\NormalTok{G)}
\NormalTok{  p1[i] <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(mod)}\OperatorTok{$}\NormalTok{coefficient[}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{]}
\NormalTok{\}}
\NormalTok{emp_power <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(p1 }\OperatorTok{<=}\StringTok{ }\NormalTok{alpha)}
\NormalTok{emp_power}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5505
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## B:}
\NormalTok{beta0 <-}\StringTok{ }\FloatTok{-0.5}
\NormalTok{betaE <-}\StringTok{ }\FloatTok{0.1}
\NormalTok{betaG <-}\StringTok{ }\FloatTok{0.3}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{2000}\NormalTok{) \{}
\NormalTok{  yB <-}\StringTok{ }\NormalTok{beta0 }\OperatorTok{+}\StringTok{ }\NormalTok{betaG}\OperatorTok{*}\NormalTok{G }\OperatorTok{+}\StringTok{ }\NormalTok{betaE}\OperatorTok{*}\NormalTok{E }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(N, }\DataTypeTok{sd =} \DecValTok{3}\NormalTok{)}
\NormalTok{  mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(yB}\OperatorTok{~}\NormalTok{E}\OperatorTok{+}\NormalTok{G)}
\NormalTok{  p2[i] <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(mod)}\OperatorTok{$}\NormalTok{coefficient[}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{]}
\NormalTok{\}}
\NormalTok{emp_power <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(p2 }\OperatorTok{<=}\StringTok{ }\NormalTok{alpha)}
\NormalTok{emp_power}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5395
\end{verbatim}

Based on the \(2000\) resampling results, the empirical powers are
basically the same in the two studies (0.55 for trait A and 0.54 for
trait B), which is consistent to the result from theoretical powers.

\hypertarget{simulation-with-logistic-regression-model}{%
\section{Simulation with logistic regression
model:}\label{simulation-with-logistic-regression-model}}

Assume the same setting as before, except now the two traits of interest
are both binary instead of continuous. Assume their generating models
are the followings:
\[\textbf{A}:\text{logit}(\text{P}(Y=1|G,E)) = -0.5 + 0.3G + 0.8E,\]
\[\textbf{B}: \text{logit}(\text{P}(Y=1|G,E)) = -0.5 + 0.3G + 0.1E.\]

All the regression parameters are the same as in the example of
continuous traits. A key difference between the generating model of
binary traits with the one of continuous traits is that there is no
nuisance parameter \(\sigma\), which already implies that there might be
a difference in their inferential results. All the studies are still
carried out on the same set of individuals.

In this example, we consider the sampling design to be prospective for
simplicity. The intercept parameter is selected to make sure the
prevalence of the trait is high enough. The explicit case-control ratio
for each trait is displayed at below:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### Simulated the common E and G}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DataTypeTok{sample.kind =} \StringTok{"Rounding"}\NormalTok{)}
\NormalTok{N <-}\StringTok{ }\DecValTok{1000}
\NormalTok{G <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DataTypeTok{size =}\NormalTok{ N, }\DataTypeTok{replace =}\NormalTok{ T, }\DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.49}\NormalTok{,}\FloatTok{0.42}\NormalTok{,}\FloatTok{0.09}\NormalTok{))}
\NormalTok{E <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(N,}\DataTypeTok{sd =} \DecValTok{3}\NormalTok{)}


\CommentTok{### Simulate each trait's disease status based on E and G}
\CommentTok{## A:}
\NormalTok{beta0 <-}\StringTok{ }\FloatTok{-0.5}
\NormalTok{betaE <-}\StringTok{ }\FloatTok{0.8}
\NormalTok{betaG <-}\StringTok{ }\FloatTok{0.3}
\NormalTok{ylat_A <-}\StringTok{ }\NormalTok{beta0 }\OperatorTok{+}\StringTok{ }\NormalTok{betaG}\OperatorTok{*}\NormalTok{G }\OperatorTok{+}\StringTok{ }\NormalTok{betaE}\OperatorTok{*}\NormalTok{E }\OperatorTok{+}\StringTok{ }\KeywordTok{rlogis}\NormalTok{(N)}
\NormalTok{y_A <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(ylat_A }\OperatorTok{>=}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\CommentTok{## B:}
\NormalTok{beta0 <-}\StringTok{ }\FloatTok{-0.5}
\NormalTok{betaE <-}\StringTok{ }\FloatTok{0.1}
\NormalTok{betaG <-}\StringTok{ }\FloatTok{0.3}
\NormalTok{ylat_B <-}\StringTok{ }\NormalTok{beta0 }\OperatorTok{+}\StringTok{ }\NormalTok{betaG}\OperatorTok{*}\NormalTok{G }\OperatorTok{+}\StringTok{ }\NormalTok{betaE}\OperatorTok{*}\NormalTok{E }\OperatorTok{+}\StringTok{ }\KeywordTok{rlogis}\NormalTok{(N)}
\NormalTok{y_B <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(ylat_B }\OperatorTok{>=}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}



\CommentTok{### Case control counts across traits:}
\NormalTok{t <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(}\KeywordTok{table}\NormalTok{(y_A),}\KeywordTok{table}\NormalTok{(y_B)) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{()}
\KeywordTok{rownames}\NormalTok{(t) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{,}\StringTok{"B"}\NormalTok{)}
\NormalTok{kableExtra}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(t, }\DataTypeTok{caption =} \StringTok{"Case Control Counts across traits"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{latex_options =} \StringTok{"HOLD_position"}\NormalTok{, }\DataTypeTok{font_size =} \DecValTok{10}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{table}[H]

\caption{\label{tab:simulatedData}Case Control Counts across traits}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{l|r|r}
\hline
  & 0 & 1\\
\hline
A & 522 & 478\\
\hline
B & 573 & 427\\
\hline
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### Case control ratio across genotypes:}
\NormalTok{t <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(y_A,y_B),}\KeywordTok{c}\NormalTok{(G,G)) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{()}
\KeywordTok{colnames}\NormalTok{(t) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Y"}\NormalTok{,}\StringTok{"G"}\NormalTok{)}
\NormalTok{t <-}\StringTok{ }\NormalTok{t }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(G) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{ratio =} \KeywordTok{sum}\NormalTok{(Y)}\OperatorTok{/}\KeywordTok{n}\NormalTok{())}
\NormalTok{kableExtra}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(t, }\DataTypeTok{caption =} \StringTok{"Case Control Ratio across genotypes"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{latex_options =} \StringTok{"HOLD_position"}\NormalTok{, }\DataTypeTok{font_size =} \DecValTok{10}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{table}[H]

\caption{\label{tab:simulatedData}Case Control Ratio across genotypes}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{r|r}
\hline
G & ratio\\
\hline
0 & 0.4238901\\
\hline
1 & 0.4683841\\
\hline
2 & 0.5200000\\
\hline
\end{tabular}
\end{table}

We can use Wald test to test the hypothesis \(\beta_G = 0\) (i.e.~G is a
casual SNP) for each trait:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## A:}
\NormalTok{mod_A <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y_A}\OperatorTok{~}\NormalTok{E }\OperatorTok{+}\StringTok{ }\NormalTok{G, }\DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link =} \StringTok{"logit"}\NormalTok{))}
\NormalTok{kableExtra}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\KeywordTok{summary}\NormalTok{(mod_A)}\OperatorTok{$}\NormalTok{coefficients, }\DataTypeTok{caption =} \StringTok{"Fitted Model for Trait A"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{latex_options =} \StringTok{"HOLD_position"}\NormalTok{, }\DataTypeTok{font_size =} \DecValTok{10}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{table}[H]

\caption{\label{tab:unnamed-chunk-1}Fitted Model for Trait A}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|)\\
\hline
(Intercept) & -0.4645384 & 0.1250578 & -3.714589 & 0.0002035\\
\hline
E & 0.8137269 & 0.0508021 & 16.017584 & 0.0000000\\
\hline
G & 0.3242170 & 0.1334258 & 2.429942 & 0.0151012\\
\hline
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## B:}
\NormalTok{mod_B <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y_B}\OperatorTok{~}\StringTok{ }\NormalTok{E }\OperatorTok{+}\StringTok{ }\NormalTok{G, }\DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link =} \StringTok{"logit"}\NormalTok{))}
\NormalTok{kableExtra}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\KeywordTok{summary}\NormalTok{(mod_B)}\OperatorTok{$}\NormalTok{coefficients, }\DataTypeTok{caption =} \StringTok{"Fitted Model for Trait B"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{latex_options =} \StringTok{"HOLD_position"}\NormalTok{, }\DataTypeTok{font_size =} \DecValTok{10}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{table}[H]

\caption{\label{tab:unnamed-chunk-1}Fitted Model for Trait B}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|)\\
\hline
(Intercept) & -0.4625221 & 0.0898641 & -5.146908 & 0.0000003\\
\hline
E & 0.0538062 & 0.0210894 & 2.551332 & 0.0107312\\
\hline
G & 0.2547557 & 0.0975127 & 2.612539 & 0.0089872\\
\hline
\end{tabular}
\end{table}

Note that the p-values are \(0.015\) for trait A, and \(0.009\) for
trait B. It is typically expected that for the trait with smaller
p-value, the magnitude of the association (i.e.~\(|\beta_G|\)) should be
larger. However, in this simulation example the true value of
\(\beta_G\) is \(\beta_G=0.3\) for both traits, and even the covariates
are exactly the same.

Again, assume that the hypothesis \(\beta_G = 0\) will be tested using
Wald test with \(\alpha=0.05\), then we can compute the theoretical
powers of the two Wald test using the simulated data \(\{G_i,E_i\}_n\)
and the true parameters vectors:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## A:}
\NormalTok{beta0 <-}\StringTok{ }\FloatTok{-0.5}
\NormalTok{betaE <-}\StringTok{ }\FloatTok{0.8}
\NormalTok{betaG <-}\StringTok{ }\FloatTok{0.3}
\CommentTok{### Theoretical Power}
\NormalTok{mod_A <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y_A}\OperatorTok{~}\NormalTok{E }\OperatorTok{+}\StringTok{ }\NormalTok{G, }\DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link =} \StringTok{"logit"}\NormalTok{))}
\CommentTok{#### Get the design matrix:}
\NormalTok{X <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,N),mod_A}\OperatorTok{$}\NormalTok{model[,}\OperatorTok{-}\DecValTok{1}\NormalTok{])}
\CommentTok{### Compute the weight matrix W:}
\NormalTok{beta <-}\StringTok{ }\KeywordTok{c}\NormalTok{(beta0,betaE,betaG)}
\CommentTok{#beta <- as.numeric(mod_A$coefficients)}
\NormalTok{w <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{N) \{}
\NormalTok{  si <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(X[i,]) }\OperatorTok{%*%}\StringTok{ }\NormalTok{beta)}
\NormalTok{  w[i] <-}\StringTok{ }\KeywordTok{dlogis}\NormalTok{(si)}
\NormalTok{\}}
\NormalTok{I <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{t}\NormalTok{(X)) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{diag}\NormalTok{(w,}\DataTypeTok{nrow =}\NormalTok{ N,}\DataTypeTok{ncol =}\NormalTok{ N) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(X)}
\CommentTok{#### Invert to get the true covariance matrix }
\NormalTok{V <-}\StringTok{ }\KeywordTok{solve}\NormalTok{(I)}
\CommentTok{### Compute the power function:}
\NormalTok{delta <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{V[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{])}\OperatorTok{*}\NormalTok{(}\DecValTok{0}\OperatorTok{-}\NormalTok{beta[}\DecValTok{3}\NormalTok{])}
\NormalTok{alpha <-}\StringTok{ }\FloatTok{0.05}
\NormalTok{Power_A <-}\StringTok{ }\DecValTok{1}\OperatorTok{-}\StringTok{ }\KeywordTok{pnorm}\NormalTok{(delta }\OperatorTok{-}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{)) }\OperatorTok{+}\StringTok{ }\KeywordTok{pnorm}\NormalTok{(delta }\OperatorTok{+}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{))}
\NormalTok{Power_A}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6193771
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## B:}
\NormalTok{beta0 <-}\StringTok{ }\FloatTok{-0.5}
\NormalTok{betaE <-}\StringTok{ }\FloatTok{0.1}
\NormalTok{betaG <-}\StringTok{ }\FloatTok{0.3}
\CommentTok{### Theoretical Power}
\NormalTok{mod_B <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y_B}\OperatorTok{~}\StringTok{ }\NormalTok{E }\OperatorTok{+}\StringTok{ }\NormalTok{G, }\DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link =} \StringTok{"logit"}\NormalTok{))}
\CommentTok{#### Get the design matrix:}
\NormalTok{X <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,N),mod_B}\OperatorTok{$}\NormalTok{model[,}\OperatorTok{-}\DecValTok{1}\NormalTok{])}
\CommentTok{### Compute the weight matrix W:}
\NormalTok{beta <-}\StringTok{ }\KeywordTok{c}\NormalTok{(beta0,betaE,betaG)}
\CommentTok{#beta <- as.numeric(mod_B$coefficients)}
\NormalTok{w <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{N) \{}
\NormalTok{  si <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(X[i,]) }\OperatorTok{%*%}\StringTok{ }\NormalTok{beta)}
\NormalTok{  w[i] <-}\StringTok{ }\KeywordTok{dlogis}\NormalTok{(si)}
\NormalTok{\}}
\NormalTok{I <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{t}\NormalTok{(X)) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{diag}\NormalTok{(w,}\DataTypeTok{nrow =}\NormalTok{ N,}\DataTypeTok{ncol =}\NormalTok{ N) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(X)}
\CommentTok{#### Invert to get the true covariance matrix }
\NormalTok{V <-}\StringTok{ }\KeywordTok{solve}\NormalTok{(I)}
\CommentTok{### Compute the power function:}
\NormalTok{delta <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{V[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{])}\OperatorTok{*}\NormalTok{(}\DecValTok{0}\OperatorTok{-}\NormalTok{beta[}\DecValTok{3}\NormalTok{])}
\NormalTok{alpha <-}\StringTok{ }\FloatTok{0.05}
\NormalTok{Power_B <-}\StringTok{ }\DecValTok{1}\OperatorTok{-}\StringTok{ }\KeywordTok{pnorm}\NormalTok{(delta }\OperatorTok{-}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{)) }\OperatorTok{+}\StringTok{ }\KeywordTok{pnorm}\NormalTok{(delta }\OperatorTok{+}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{))}
\NormalTok{Power_B}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8618099
\end{verbatim}

Based on the results above, we know in this simulation study, the power
of Wald test will be \(0.619\) for the trait A, and \(0.861\) for the
trait B. Note that Wald test on trait B has quite larger power compared
to on trait A, despite the fact that the two samples are generated with
same \(\beta_G = 0.3\) and generated by the same set of
\(\{G_i,E_i\}_n\). This suggests the p-values of Wald test may have very
different distributions on the two traits. We can double check that our
theoretical powers for both tests are correct using empirical powers:

To compute the empirical powers, we re-simulated each type of binary
trait for \(K = 2000\) times, and compute the \(2000\) p-values in each
trait:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DataTypeTok{sample.kind =} \StringTok{"Rounding"}\NormalTok{)}
\CommentTok{## A:}
\NormalTok{beta0 <-}\StringTok{ }\FloatTok{-0.5}
\NormalTok{betaE <-}\StringTok{ }\FloatTok{0.8}
\NormalTok{betaG <-}\StringTok{ }\FloatTok{0.3}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{2000}\NormalTok{) \{}
\NormalTok{  ylat_A <-}\StringTok{ }\NormalTok{beta0 }\OperatorTok{+}\StringTok{ }\NormalTok{betaG}\OperatorTok{*}\NormalTok{G }\OperatorTok{+}\StringTok{ }\NormalTok{betaE}\OperatorTok{*}\NormalTok{E }\OperatorTok{+}\StringTok{ }\KeywordTok{rlogis}\NormalTok{(N)}
\NormalTok{  y_A_rep <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(ylat_A }\OperatorTok{>=}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{  mod <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y_A_rep}\OperatorTok{~}\NormalTok{E}\OperatorTok{+}\NormalTok{G, }\DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link =} \StringTok{"logit"}\NormalTok{))}
\NormalTok{  p1[i] <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(mod)}\OperatorTok{$}\NormalTok{coefficient[}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{]}
\NormalTok{\}}
\NormalTok{emp_power <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(p1 }\OperatorTok{<=}\StringTok{ }\NormalTok{alpha)}
\NormalTok{emp_power}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6115
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## B:}
\NormalTok{beta0 <-}\StringTok{ }\FloatTok{-0.5}
\NormalTok{betaE <-}\StringTok{ }\FloatTok{0.1}
\NormalTok{betaG <-}\StringTok{ }\FloatTok{0.3}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{2000}\NormalTok{) \{}
\NormalTok{  ylat_B <-}\StringTok{ }\NormalTok{beta0 }\OperatorTok{+}\StringTok{ }\NormalTok{betaG}\OperatorTok{*}\NormalTok{G }\OperatorTok{+}\StringTok{ }\NormalTok{betaE}\OperatorTok{*}\NormalTok{E }\OperatorTok{+}\StringTok{ }\KeywordTok{rlogis}\NormalTok{(N)}
\NormalTok{  y_B_rep <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(ylat_B }\OperatorTok{>=}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{  mod <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y_B_rep}\OperatorTok{~}\NormalTok{E}\OperatorTok{+}\NormalTok{G, }\DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link =} \StringTok{"logit"}\NormalTok{))}
\NormalTok{  p2[i] <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(mod)}\OperatorTok{$}\NormalTok{coefficient[}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{]}
\NormalTok{\}}
\NormalTok{emp_power <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(p2 }\OperatorTok{<=}\StringTok{ }\NormalTok{alpha)}
\NormalTok{emp_power}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8635
\end{verbatim}

Based on the \(2000\) resampling results, the empirical powers are
respectively 0.612 for trait A and 0.864 for trait B. These values are
quite close to the theoretical values \(0.619\) and \(0.861\) we
computed above. The distributions of p-values in each trait can be
visualized as well:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### Comparison:}
\NormalTok{pcomp <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{P =} \KeywordTok{c}\NormalTok{(p1,p2), }\DataTypeTok{trait =} \KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\StringTok{"A"}\NormalTok{,}\DecValTok{2000}\NormalTok{),}\KeywordTok{rep}\NormalTok{(}\StringTok{"B"}\NormalTok{,}\DecValTok{2000}\NormalTok{)))}
\NormalTok{pcomp }\OperatorTok{%>%}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ P, }\DataTypeTok{fill =}\NormalTok{ trait)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{20}\NormalTok{, }\DataTypeTok{alpha=}\FloatTok{0.5}\NormalTok{, }\DataTypeTok{position=}\StringTok{"identity"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{GWAS-Pvalues_files/figure-latex/visualization-1.png}

Based on the figure above, we can conclude that the distribution of p
values in trait B is stochastically smaller than the distribution in
trait A, even if their underlying \(\beta_G\) are both \(0.3\).
Therefore, it shows that the magnitudes of p-values of different studies
are not directly comparable, unless the generalized linear regression
model being used is the ordinary linear regression model with \(g\)
being identity function.

\clearpage

\hypertarget{bibliography}{%
\section{Bibliography}\label{bibliography}}

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}

\noindent

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-meta1}{}%
Begum, Ferdouse, Debashis Ghosh, George C. Tseng, and Eleanor Feingold.
2012. ``Comprehensive literature review and statistical considerations
for GWAS meta-analysis.'' \emph{Nucleic Acids Research} 40 (9):
3777--84.

\leavevmode\hypertarget{ref-meta2}{}%
Evangelou, Evangelos, and John Ioannidis. 2013. ``Evangelou E, Ioannidis
Jp.meta-Analysis Methods for Genome-Wide Association Studies and Beyond.
Nat Rev Genet 14:379-389.'' \emph{Nature Reviews. Genetics} 14 (May).

\leavevmode\hypertarget{ref-MTAG}{}%
Turley, Patrick, Raymond K Walters, Omeed Maghzian, Aysu Okbay, James J
Lee, Mark Alan Fontana, Tuan Anh Nguyen-Viet, et al. 2018. ``Multi-Trait
Analysis of Genome-Wide Association Summary Statistics Using Mtag.''
\emph{Nature Genetics} 50 (2): 229--37.

\end{document}
